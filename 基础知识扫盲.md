# 特征归一化、标准化

**作用**：

1. 使不同量纲的特征处于同一数值量级，减少方差大的特征的影响，使模型更准确；
2. 加快学习算法的收敛速度;

**常用方法**：

1. 线性归一化（min-max标准化）；
2. x’ = (x-min(x)) / (max(x)-min(x))，其中max是样本数据的最大值，min是样本数据的最小值；
3. 适用于数值比较集中的情况，可使用经验值常量来来代替max，min；
4. 标准差归一化（z-score 0均值标准化）；
5. 

# k-means 和knn

![](D:\StudyFiles\MarkAll\Interview\基础知识扫盲\20210304203226.png)

# 加快网络的收敛

增大学习率

# 过拟合、欠拟合

**过拟合**的原因：训练数据不够多，或者over-training（过度训练），导致在训练集上error减小，而在验证集上error逐渐增大；

解决方法： 

1.  正则化：在损失函数中加入正则项来惩罚模型的参数，以此来降低模型的复杂度，常见的添加正则项的正则化技术有L1，L2正则化 ；
2.  提前终止迭代（Early stopping）：随着迭代次数的增大，部分权值参数会不断的增大。提前终止迭代可以有效的控制权值参数的大小，从而降低模型的复杂度。；
3. 权值共享：权值共享的目是为了减小模型中的参数，同时还能减少计算量。 

 4、数据增强，也就是增加训练数据样本。 

 5、Dropout：在神经网络中，随机地使一部分隐层单元暂时性失活。补充：在前向传播的时候，让某个神经元的激活值以一定的概率停止工作，即输出为0，也不更新参数，这样可以使模型泛化能力更强，因为它不会太依赖某些局部的特征。 

 6、剪枝：剪枝是决策树中一种控制过拟合的方法。预剪枝：通过在训练过程[中控]()制树深、叶子节点数、叶子节点中样本的个数等来控制树的复杂度，就是提前停止训练。后剪枝：是在训练好树模型之后，采用交叉验证的方式进行剪枝以找到最优的树模型。 

 


 **欠拟合**的原因：模型学习不足，没有很好地捕捉到数据特征，不能够很好地拟合数据。或者是因为模型太简单了。 

 解决方法：增加特征，使用组合特征，减少正则化参数。

# Batch Normalization

Normalization：将一组范围差距较大或者量级不同的数据，按照一定的规则变化到指定的范围内 

BN主要作用是确保网络中的各层，**即使参数发生了变化， 其输入/输出数据的分布也不能产生较大变化，从而避免发生内部协变量偏移现象；**

 BN的原因：随着网络的深度增加，每一层的特征值分布会逐渐向激活函数的饱和区间靠近，这样就会导致梯度消失。BN能够将当前层的特征值分布重新拉回标准正态分布，特征值会集中在激活函数的敏感区间，就算输入的变化较小也可导致损失函数的较大变化，使得梯度变大，避免梯度消失，同时也可加快收敛。（一般是放在激活函数之前吧？） 

- 网络每一层需要不断适应输入数据的分布的变化，这会影响学习效率，并使学习过程变得不稳定。
- 网络前几层参数的更新，很可能使得后几层的输入数据变得过大或者过小，从而掉进激活函数的饱和 区，导致学习过程过早停止。
- 为了尽量降低内部协变量偏移带来的影响，网络参数的更新需要更加谨慎，在实际应用中一般会采用 较小的学习率（避免参数更新过快），而这会降低收敛速度。

公式：
$$
y^{(k)}=\gamma^{(k)}\frac{x^{(k)}-\mu^{(k)}}{\sqrt{(\sigma^{(k)})^{2}+\epsilon}}+\beta^{(k)}
$$
 增加两个调节参数
$$
\beta 和 \gamma
$$
的作用：

- 保留网络各层在训练过程中的学习成果。如果没有这两个参数，批归一化退化为普通的标准化，这样在训练 过程中，网络各层的参数虽然在更新，但是它们的输出分布却几乎不变（始终是均值为0、标准差为 1），不能有效地进行学习。添加这两个参数后，网络可以为每个神经元自适应地学习一个量身定制的 分布（均值为beta 、标准差为gamma ），保留每个神经元的学习成果；
- 保证激活单元的非线性表达能力。上面提到，没有beta和gamma，批归一化的输出分布始终是均值为0、标准 差为1。此时，如果激活函数采用诸如Sigmoid、Tanh等函数，则经过批归一化的数据基本上都落在这 些激活函数的近似线性区域，没能利用上它们的非线性区域，这会极大地削弱模型的非线性特征提取 能力和整体的表达能力。添加这两个参数后，批归一化的数据就可以进入激活函数的非线性区域；
- 使批归一化模块具有自我关闭能力。若beta和gamma分别取数据的均值和标准差，则可以复原初始的输入值， 即关闭批归一化模块。因此，当批归一化导致特征分布被破坏，或者使网络泛化能力减弱时，可以通过这两个参数将其关闭。

# 反向传播

反向传播误差，求误差和对权重w的偏导，求偏导一般没法直接求，参考链式法则。

# 决策树和随机森林

**决策树**是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，每个叶节点代表一种分类结果。

决策树构造：

1. 节点的熵迅速降低；
2. 熵降低的速度越快越好；
3. 得到一颗最矮的决策树。

**信息增益**gain：原始熵 - 当前熵；

**D3算法**：信息增益最大化；缺点：考虑ID，每个ID出现一次，所以算出的信息增益大，但是没有意义

**信息增益率**：信息增益/当前特征的熵

**回归决策树**：是将特征空间划分成若干单元，每一个单元有一个特定的输出。[切分点选择：最小二乘法]；[输出值：单元内均值]. 

**随机森林**:

Bootstrapping: 有放回的均匀采样;

Bagging：有放回的采样得到n个样本，建立n个分类器 ;

一次采样构成一颗决策树，多次采样，构成一片森林，多个分类器共同决定结果。当输入一个测试样例时，输入到所有的决策树，共同决策。

 随机性的解释： 

1. 数据选择的随机性：有放回的选择数据集，这样可以有概率的忽略错误数据 
2. 特征的随机选择   

# IoU交并比

IOU=(A∩B)/(A∪B) 

# 给图像添加噪声

1. 椒盐噪声，椒盐噪声（脉冲噪声）是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）；

2. 高斯噪声 ，高斯噪声是指概率密度函数服从高斯分布的一类噪声。高斯噪声的每个像素点都出现噪声。 

   ​    如果一个噪声，它的幅度分布服从高斯分布，而它的功率谱密度服从均匀分布，则称这个噪声为高斯白噪声。 

# 图像的特征

1. 颜色特征（颜色直方图）
2. 形状特征（Sift、hog）
3. 纹理特征（LBP）
4. 边缘特征（   canny、   sobel）  

# 双线性插值

双线性插值，又称为双线性内插。在数学上双线性插值是有两个变量的插值函数的线性插值扩展，其核心思想是在两个方向分别进行一次线性插值。

**单线性插值**

已知数据 (x0, y0) 与 (x1, y1)，要计算 [x0, x1] 区间内某一位置 x 在直线上的y值。

![](D:\StudyFiles\MarkAll\Interview\基础知识扫盲\QQ截图20210306154154.png)
$$
y = \frac{x_1-x}{x_1-x_0}y_0+\frac{x-x_0}{x_1-x_0}y_1
$$
**双线性插值法**0

![](D:\StudyFiles\MarkAll\Interview\基础知识扫盲\QQ截图20210306155432.png)

假如我们想得到未知函数 f 在点 P = (x, y) 的值，假设我们已知函数 f 在 Q11 = (x1, y1)、Q12 = (x1,  y2), Q21 = (x2, y1) 以及 Q22 = (x2, y2) 四个点的值。最常见的情况，f就是一个像素点的像素值。首先在 x  方向进行线性插值，得到
$$
f(R_1)\approx\frac{x_2-x}{x_2-x1}f(Q_{11})+\frac{x-x_1}{x_2-x_1}f(Q_{21}) \;\;where\;\;R_1=(x,y_1)
$$

$$
f(R_2)\approx\frac{x_2-x}{x_2-x1}f(Q_{12})+\frac{x-x_1}{x_2-x_1}f(Q_{22}) \;\;where\;\;R_2=(x,y_2)
$$

然后在 y 方向进行线性插值，得到 
$$
f(P)\approx\frac{y_2-y}{y_2-y1}f(R_{1})+\frac{y-y_1}{y_2-y_1}f(R_{2}) 
$$



# Dropout

Dropout是神经网络中非常重要的缓解过拟合的方法，是在2012年的AlexNet中提出的[5]。在卷积神经网络 中，Dropout是非常有效的正则化方法;

# 深度可分离卷积

输入特征：DF X DF X M ；输出特征：DF X DF X N；

卷积过程：

- 深度卷积：卷积核通道数为1，只负责一个通道；
- 点卷积：逐点卷积的卷积核大小为1x1xM，每次卷积一个像素，将上一步的特征图在深度方向上进行加权组合，生成新的特征图；

对比标准卷积：每个卷积核的通道与输入通道相同。每个通道单独做卷积运算后相加；

算力消耗比较：

- 深度可分离卷积：DK X DK X M X DF X DF + M X N X DF X DF （深度卷积 + 点卷积）
- 标准卷积：DK X DK X N X M X DF X DF；

所以两者之比为 1/N + 1/(DK X DF)；

# 自适应池化(Adaptive Pooling)

[输出大小固定](https://blog.csdn.net/u013382233/article/details/85948695)

# Mobile Net 系列对比

[参考博客](https://junchu.blog.csdn.net/article/details/109460784)

## v1

|      创新      |               作用               |
| :------------: | :------------------------------: |
| 深度可分离卷积 | 代替常规卷积，减少参数量和运算量 |
|   宽度超参数   |        改变输入输出通道数        |
|  分辨率超参数  |        改变输入特征的尺寸        |

## v2

|                             创新                             |                    作用                    |
| :----------------------------------------------------------: | :----------------------------------------: |
| 线性瓶颈结构（Linear Bottleneck）[在卷积降维后面不再加入ReLu，因为在低维中Relu会破坏特征] |             减少参数量，运算量             |
|    逆残差结构（先1x1卷积升维，再3x3卷积，再1x1卷积降维）     | 加深网络层数，避免梯度消失问题，减少参数量 |
|                      平均池化与逐点卷积                      |           去掉全连接层，减少参数           |

## v3

|                             创新                             |                  作用                  |
| :----------------------------------------------------------: | :------------------------------------: |
|                       h-swish激活函数                        |         保持精度情况下加快速度         |
| 引入[SENet](https://blog.csdn.net/qq_42617455/article/details/108165206)结构（压缩-激活） | 建模通道间的关系，提升模型特征表达能力 |
|                           5x5卷积                            |        用5x5代替3x3，准确率更高        |

SEnet中使用[全局池化](https://blog.csdn.net/fjsd155/article/details/88953153)

**relu6**: y= min(max(0,x), 6)

h-swish(x) = x * [relu6( x + 3)] / 6

# 经典CNN网络解析

[AlexNet、VGG、NIN、GoogLeNet、ResNet etc.](https://zhuanlan.zhihu.com/p/47391705)

# 经典分割网络

[FCN、SegNet、UNet、LinkNet、PSPNet、DeepLab系列](https://blog.csdn.net/qq_37002417/article/details/108274404)